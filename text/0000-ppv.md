- Feature Name: `portable_packed_vector_types`
- Start Date: (fill me in with today's date, YYYY-MM-DD)
- RFC PR: (leave this empty)
- Rust Issue: (leave this empty)

# Summary
[summary]: #summary

This RFC adds portable packed SIMD vector types up to 256-bit.

Future RFCs will cover some of the following extensions as they become finished
in `stdsimd`:

* portable vector shuffles, gathers, and scatters
* boolean vectors, boolean reductions, portable vector comparisons

and will also attempt to answer some of the unresolved questions.

# Motivation
[motivation]: #motivation

The `std::arch` module exposes architecture-specific SIMD types like `_m128` - a
128-bit wide SIMD vector type. How these bits are interpreted depends on the intrinsics
being used. For example, let's sum 8 `f32`s values using the SSE4.1 facilities
in the `std::arch` module. This is one way to do it
([playground](https://play.rust-lang.org/?gist=165e2886b4883ec98d4e8bb4d6a32e22&version=nightly)):

```rust
unsafe fn add_reduce(a: __m128, b: __m128) -> f32 {
    let c = _mm_hadd_ps(a, b);
    let c = _mm_hadd_ps(c, _mm_setzero_ps());
    let c = _mm_hadd_ps(c, _mm_setzero_ps());
    std::mem::transmute(_mm_extract_ps(c, 0))
}

fn main() {
    unsafe {
        let a = _mm_set_ps(1., 2., 3., 4.);
        let b = _mm_set_ps(5., 6., 7., 8.);
        let r = add_reduce(a, b);
        assert_eq!(r, 36.);
    }
}
```

Notice that:

* one has to put some effort to extrapolate from `add_reduce`'s signature what
  types of vectors it actually expects: "`add_reduce` takes 128-bit wide vectors and
  returns a `f32` therefore those 128-bit vectors _probably_ must contain 4 packed
  f32s because that's the only combination of `f32`s that fits in 128 bits!"
  
* it requires a lot of `unsafe` code: the intrinsics are unsafe, the intrinsic
  API relies on the user performing transmutes, constructing the vectors is
  unsafe because it needs to be done via intrinsic calls, etc.

* it requires a lot of architecture specific knowledge: how the intrinsics are
  called, how they are used together
  
* this solution only works on `x86` or `x86_64` with SSE4.1 enabled, that is, it
  is not portable.

With portable packed vector types we can do much better
([playground](https://play.rust-lang.org/?gist=7fb4e3b6c711b5feb35533b50315a5fb&version=nightly)):

```rust
fn main() {
    let a = f32x4::new(1., 2., 3., 4.);
    let b = f32x4::new(5., 6., 7., 8.);
    let r = (a + b).sum();
    assert_eq!(r, 36.);
}
```

In both cases, for `x86_64`, Rust generates the exact same assembly instruction
sequence:

```asm
  haddps xmm0, xmm1
  xorps xmm1, xmm1
  haddps xmm0, xmm1
  haddps xmm0, xmm1
```

And this is key: these types add zero-overhead over the architecture-specific
types for the operations that they support - if there is an architecture in
which this does not hold for some operation: the implementation has a bug.

TODO: @sunfish this implies that in some arch we might need to lower `(a +
b).sum()` into a single instruction. That is, every single piece of this API
must be something that the backend understands :/

TODO: @alex an alternative would be to not say anything here about
zero-overhead, that is, while we might be able to guarantee some overhead in
some cases, we can't really work around backed bugs in lowering combinations of
operations like `(a + b).sum()`.

# Guide-level explanation
[guide-level-explanation]: #guide-level-explanation

This RFC introduces portable packed SIMD vector types and their minimal API. Here:

* portable: is the opposite of architecture-specific. These types work both
  correctly and efficiently on all architectures. They are a zero-overhead
  abstraction, that is, for the operations that these types support, one cannot
  write better code by hand (otherwise it is an implementation bug).
  
* packed: is the opposite of "scalable" or "Cray vectors". That is, these
  vector's have a compile-time fixed size. Therefore, they can be stored inside
  structs, on the stack, and on the heap.
  
Packed vector types are named as follows: `{i,u,f}{lane_width}x{#lanes}`, so that
`i64x8` is a 512-bit vector with eight `i64` lanes and `f32x4` a 128-bit vector
with four `f32` lanes.

Operations on vector types can be either:

* vertical: that is, lane-wise. For example, `a + b` adds each lane of `a` with
  the corresponding lane of `b`, while `a.lt(b)` returns a vector of "bools"
  indicating whether the `lt` (less-than: `<`) for each lane returned `true` or
  `false`. Most vertical operations are binary operations (they take two input
  vectors). These operations are typically very fast - this is how vectors should
  be used.
  
* horizontal: that is, along a vector. Horizontal operations are unary, that is,
  they take one input vector. For example, `a.sum()` adds the elements of the
  vector together and `a.max()` returns the largest element in the vector. These
  operations are typically slower than vertical operations but in some cases
  they are necessary.  
  
# Reference-level explanation
[reference-level-explanation]: #reference-level-explanation
  
## Vector types

The vector types are named according to the following scheme:

> {element_type}{lane_width}x{number_of_lanes}

where the following element types are introduced by this RFC:

* `i`: signed integer
* `u`: unsigned integer
* `f`: float

So that `u16x8` reads "a SIMD vector of eight packed 16-bit wide unsigned
integers". The width of a vector can be computed by multiplying the
`{lane_width}` times the `{number_of_lanes}`. For `u16x8`, 16 x 8 = 128, so
this vector type is 128 bits wide.

> Note: this nomenclature follows closely the nomenclature used by ARM in their
> vector types: <type><size>x<number of lanes>_t

This RFC proposes adding all vector types with sizes in range [16, 256] bit to
the `std::simd` module, that is:

* 16-bit wide vectors:
  * `i8x2`
  * `u8x2`
* 32-bit wide vectors:
  * `i8x4`
  * `u8x4`
  * `i16x2`
  * `u16x2`
* 64-bit wide vectors:
  * `i8x8`
  * `u8x8`
  * `i16x4`
  * `u16x4`
  * `i32x2`
  * `u32x2`
  * `f32x2`
* 128-bit wide vectors:
  * `i8x16`
  * `u8x16`
  * `i16x8`
  * `u16x8`
  * `i32x4`
  * `u32x4`
  * `f32x4`
  * `i64x2`
  * `u64x2`
  * `f64x2`

* 256-bit wide vectors:
  * `i8x32`
  * `u8x32`
  * `i16x16`
  * `u16x16`
  * `i32x8`
  * `u32x8`
  * `f32x8`
  * `i64x4`
  * `u64x4`
  * `f64x4`

Note that:

* half-float (`f16`) vectors (and half-floats in general) will need to be
  covered by future RFCs to support ARM/AArch64/PowerPC64/... intrinsics
* 512-bit wide vectors and masks will need to be covered once `std::arch` adds AVX-512 support
* vectors with 128-bit wide elements (`{i,u,f}128x1`, `{i,u,f}128x2`) will need
  to be covered once `std::arch` adds PowerPC support.

## API of portable packed SIMD vector types

### Traits overview

All signed integer, unsigned integer, and floating point vector types implement
the following traits:

* `Copy`
* `Clone`
* `Default`: zero-initializes the vector.
* `Debug`: formats the vector as `({}, {}, ...)`.
* `PartialEq<Self>`: performs a lane-wise comparison between two vectors and
  returns `true` if all lanes compare `true`.
* `PartialOrd<Self>`: performs a lane-wise comparison between two vectors and
  returns `true` if all lanes compare `true`. TODO: we should actually perform a
  lexicographical order here. Does this make sense? It's not going to be
  efficient. What we currently do IIRC is `a.lt(b).all()`.
* `From`/`Into`/`FromBits`/`IntoBits`: see the [lane-wise casts and bitwise
  conversions](#casts-and-conversions) section below.
* `Add<Output=Self>`, `Sub<Output=Self>`, `Mul<Output=Self>`, 
  `Div<Output=Self>`, `Rem<Output=Self>`, `AddAssign`, `SubAssign`, `MulAssign`, 
  `DivAssign`, `RemAssign`: lane-wise arithmetic operations.
* TODO: arithmetic operations with vector's element type: `Add<f32> for f32x4`.
  Can probably be initially implemented as splat + vector op.

All signed and unsigned integer vectors also implement:

* `Eq`: equivalent to `PartialEq<Self>`
* `Ord`: equivalent to `PartialOrd<Self>`
* `Hash`: equivalent to `Hash` for `[element_type; number_of_elements]`.
* `fmt::LowerHex`: formats the vector as hexadecimal.
* TODO: `fmt::UpperHex`: formats the vector as hexadecimal.
* TODO: `fmt::Octal`: formats the vector as an octal number.
* TODO: `fmt::Binary`: formats the vector as binary number.
* `Not<Output=Self>`,`BitAnd<Output=Self>`, `BitOr<Output=Self>`,
  `BitXor<<Output=Self>`, `BitAndAssign`, `BitOrAssign`,
  `BitXorAssign`: lane-wise bitwise operations.
* TODO: bitwise operations with vector element type: `BitAnd<u32> for u32x4`.
  can probably be initially implemented as splat + vector op.
* TODO: shifts for vector type: `Shl<Self>` and friends
* `Shl<T>`, `Shr<T>`, `ShlAssign<T>`,`ShrAssign<T>`: for all types `T` in {`i8`, `i16`,
  `i32`, `i64`, `i128`, `isize`, `u8`, `u16`, `u32`, `u64`, `u128`, `usize`}.

### Inherent Methods

#### Construction and element access

All portable vector types implement the following methods:

```rust
impl {element_type}{lane_width}x{number_of_lanes} {

/// Creates a new instance of the vector from `number_of_lanes` 
/// values.
pub const fn new(args...) -> Self;

/// Returns the number of vector lanes.
pub const fn lanes() -> usize;

/// Constructs a new instance with each element initialized to
/// `value`.
pub const fn splat(value: element_type) -> Self;

/// Extracts the value at `index`.
///
/// # Panics
///
/// If `index >= Self::lanes()`.
pub fn extract(self, index: usize) -> element_type;

/// Extracts the value at `index`.
///
/// If `index >= Self::lanes()` the behavior is undefined.
pub unsafe fn extract_unchecked(self, index: usize) -> element_type;

/// Returns a new vector where the value at `index` is replaced by `new_value`.
///
/// # Panics
///
/// If `index >= Self::lanes()`.
#[must_use = error-message]
pub fn replace(self, index: usize, new_value: $elem_ty) -> Self;

/// Returns a new vector where the value at `index` is replaced by `new_value`.
#[must_use = error-message]
pub unsafe fn replace_unchecked(self, index: usize, 
                                new_value: element_type) -> Self;

}
```

TODO: @sunfish commented that `new` lowers to a memory load even when all
arguments are constant. This is inefficient, and probably something to be
discouraged (people should be doing a load here, or a sequence of replaces). 

OTOH @gnzlbg finds this useful, but that might be because it is useful inside
`stdsimd` to write tests and short examples, but arguably it is not something
used by high-performance code.

It might thus make sense to:

* have a way to construct these from compile-time constants
* use that way to construct these when all arguments to `new` are constants
* either require the arguments of `new` to be constants, or add a way that
  requires this, like a literal syntax for the simd types: `f32x4(1., 2., 3.,
  4.)` that requires all input arguments to be compile-time constants.
  
Skimming through `stdsimd` I can't find a single place where `new` is used with
non-constant arguments.

TODO: @sunfish mentions that he is not aware of any hardware that supports
dynamic indexing into vector elements:

> It's not horrible, but it's not a SIMD operation. It's a memory operation, and
> Rust can already do it. Just create a [f32; 4], store the vector there, index it
> yourself, and the compiler should generate the same code. If this were a common
> operation, it'd be nice to have a convenient API to do it automatically, but as
> far as I know it isn't (otherwise we might have hardware for it ;-)). And that
> way, you don't need the `_unchecked` versions, because users can do whatever
> unsafe thing they want with memory themselves.

So in a nutshell we might want to require the arguments of `new` and
`extract`/`store`/`replace` to be `const`s. The problem would be that we can't
use the `constify!` techniques here that the immediate-mode arguments use in
`std::arch`, so maybe we'll need to make these be macros for now.

#### Lane-wise casts and bitwise conversions
[casts-and-conversions]: #casts-and-conversions

The portable vector types implement lane-wise and bitwise casts.

##### Lane-wise casts

TODO: `From`/`Into` are lossless. What we actually want here is `as` casts.
Implementing `From`/`Into`` here with a different meaning is not going to
survive the RFC process. The alternative was `.as_f32x4()` methods, but that was
painful to implement and use.

Vectors with identical number of lanes can be converted into each other by
performing lane-wise `as` casts provided by the `From` and `Into` traits. 

For example:

```rust
let x = f32x2::new(1., 2.);
let y: f64x2 = x.into();
```

These conversions are provided across all portable vector types having the same
number of lanes.

##### Bitwise casts

TODO: `f32::from_bits` takes a `u32`, if we were to add these as methods we'd
need to add a bunch of them for each type as well `__m128::from_bits_{f32x4,
...}`..

To interface with the architecture-specific vector types and intrinsics it is
often required to perform lossless bitwise casts from one type to another.

In an analogous way to the `f32::from_bits` method and the `From` and `Into`
traits, these conversions are performed via the reflexive `FromBits`/`IntoBits`
traits. These conversions have run-time zero-overhead.

For example:

```rust
let x = f32x4::new(1., 2., 3., 4.);
let y: __m128 = x.into_bits();
```

These conversions are provided across all portable vector types of the same
width, and across the portable vector types and the architecture-specific vector
types.

TODO: @sunfish gave the following comment: If the bitwise casts can convert
between vectors with different numbers of elements (eg. between `i8x16` and
`i16x8`), the results will depend on the endianness of the machine.

#### Loads and Stores

All portable vector types implement the following methods:

```rust
impl {element_type}{lane_width}x{number_of_lanes} {

/// Writes the values of the vector to the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not
/// aligned to an `align_of::<Self>()` boundary.
pub fn store_aligned(self, slice: &mut [element_type]);

/// Writes the values of the vector to the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()`.
pub fn store_unaligned(self, slice: &mut [element_type]);

/// Writes the values of the vector to the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not
/// aligned to an `align_of::<Self>()` boundary, the behavior is
/// undefined.
pub unsafe fn store_aligned_unchecked(self, slice: &mut [element_type]);

/// Writes the values of the vector to the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` the behavior is undefined.
pub unsafe fn store_unaligned_unchecked(self, slice: &mut [element_type]);

/// Instantiates a new vector with the values of the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not aligned
/// to an `align_of::<Self>()` boundary.
pub fn load_aligned(slice: &[element_type]) -> Self;

/// Instantiates a new vector with the values of the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()`.
pub fn load_unaligned(slice: &[element_type]) -> Self;

/// Instantiates a new vector with the values of the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not aligned
/// to an `align_of::<Self>()` boundary, the behavior is undefined.
pub unsafe fn load_aligned_unchecked(slice: &[element_type]) -> Self;

/// Instantiates a new vector with the values of the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` the behavior is undefined.
pub unsafe fn load_unaligned_unchecked(slice: &[element_type]) -> Self;

}
```

TODO: @sunfish gave the following comment: need to document how big-endian does
things.

#### Arithmetic reductions 

All portable vector types implement the following methods:

```rust
impl {element_type}{lane_width}x{number_of_lanes} {

/// Horizontal sum of the vector elements.
pub fn sum(self) -> element_type;

/// Horizontal product of the vector elements.
pub fn product(self) -> element_type;

}
```

##### Semantics for integer vectors

For integer vectors, if integer overflow occurs the result is the same as for
overflow with regular integers when no `-C debug-assertions` are enabled. 

In most architectures this corresponds to the mathematical result modulo `2^n`.

TODO: I am a bit uncomfortable with integers overflowing here. We should
probably offer the same behavior as for non-vector integers, and offer ways to
opt into wrapping arithmetic.

##### Semantics for floating point vectors.

For floating-point vectors, the floating-point horizontal reductions are ordered
(TODO), that is, equivalent to the following Rust code:

TODO: see below for a `#[fast_math(...)]` alternative approach to this, e.g.,
`#[fast_math(assume = "associativity")]` could use a different ordering.

```rust
// TODO
```

TODO: @sunfish commented that ordered reductions can be made pretty efficient by
doing a tree reduction (VPADD on NEON, HADDPS on x86): `(x[0]+x[1])+(x[2]+x[3])`
but nothe that this is not equivalent to `x[0] + x[1] + x[2] + x[3]` which is
what the following code would do:

```rust
let mut val: f32 = vec.extract(0);
for i in 1..f32x4::lanes() {
    val += vec.extract(i);
}
assert_eq!(val, vec.sum()); // MAYBE FAILS
```

TODO: AFAICT the LLVM reductions don't do a tree reduction, but I should
triple-check this.


#### Bitwise reductions 

All signed and unsigned integer vectors implement the following methods:

```rust
impl {element_type}{lane_width}x{number_of_lanes} {

/// Horizontal bitwise `and` of the vector elements.
pub fn and(self) -> element_type;

/// Horizontal bitwise `or` of the vector elements.
pub fn or(self) -> element_type;

/// Horizontal bitwise `xor` of the vector elements.
pub fn xor(self) -> element_type;

}
```

#### Min/Max reductions

All portable vector types implement the following methods:

```rust
impl {element_type}{lane_width}x{number_of_lanes} {
            
/// Value of the largest element in the vector.
pub fn max(self) -> element_type;

/// Value of the smallest element in the vector.
pub fn min(self) -> element_type;

}
```

##### Semantics for floating-point vectors

If the vector contains `NaN`s the result is unspecified (TODO). 

TODO: @sunfish would prefer to handle `NaNs` in Min/Max by default and, if
desired, adding a version that optimizes for the non-`NaN`s case as an
alternative.

TODO: an alternative is to offer a Rust API here, and to provide fast-math via a
different mechanism, like a crate/module/function/block-scoped attribute:
`#[fast_math(assume="all", require="associativity")]` to enable all fast-math
assumptions except for associativity, or `#[fast_math(assume="finite")]` to only
enable fast-math that assumes finite-arithmetic (e.g. no `NaN`s). I think
something like this is something that Rust might want to expose anyways.

If the vector contains a `NaN`, then both `min`/`max` would return it:

```rust
let nan: f32 = 1. / 0.;
let vec = f32x4::new(1., 2., nan, 3.);
assert_eq!(vec.max() as u32, nan as u32);
assert_eq!(vec.min() as u32, nan as u32);
```

The problem is what do we do when the vector has multiple `NaN`s ? That is, given

```rust
let nan_0: f32 = ...some nan...;
let nan_1: f32 = ...some nan...;
let nan_2: f32 = ...some nan...;
let nan_3: f32 = ...some nan...;
assert!(nan_0 as u32 != nan_1 as u32 && nan_0 as u32 != nan_2 as u32 &&
        nan_0 as u32 != nan_3 as u32);
assert!(nan_1 as u32 != nan_2 as u32 && nan_1 as u32 != nan_3 as u32);
assert!(nan_2 as u32 != nan_3 as u32);
let vec = f32x4::new(nan_0, nan_1, nan_2, nan_3);
asset_eq!(vec.max() as u32, ???);
asset_eq!(vec.min() as u32, ???);
```

We should check how to implement these manually for, e.g., f32x4 on AArch64,
x86_64, and PowerPC64 and see how the fastest code behaves in the presence of
NaNs, and what would it take to homogenize the behavior here. The results would
then belong in the rationale section.

# TODO: ABI 

TODO: @acrichto: IIRC currently the Rust ABI for SIMD types always passes
vector types by pointers. This avoids the issues mentioned in:
https://github.com/rust-lang/rust/issues/44367

How exactly this work is pretty important and should be explained here. 

Then we have the issue that SIMD types in `extern` functions might not be sound.
IIRC this is currently disallowed. Is there an issue for this? 

# Drawbacks
[drawbacks]: #drawbacks

The main drawback is that in a future where Rust has multiple backends,
providing this types will take some work. Most backeds support vector types in
one form or another, and if the backend supports the architecture-specific
vector types exposed by `std::arch` chances are that very little work will
actually be required to support this.

TODO: @sunfish? However, the zero-overhead requirement requires the backend to
be able to optimize sequences of operations in architectures that support them.
For example, imagine that a future architecture would provide a `add_and_reduce`
instruction for float vectors. In that architecture, `(a + b).sum()` would need
to be lowered to that single instruction. This is not something that can be
worked around in a library implementation, but rather something that needs
compiler and backend support.

TODO: @sunfish? Another drawback is that depending on the features enabled and
the architecture, some of the operations exposed here are slow. As in, the
backed will currently generate scalar code that operates on the vector using
extract + replace. Is it possible for backends to transform vector operations
back to scalar code that's "efficient" (e.g. a loop)?

# Rationale and alternatives
[alternatives]: #alternatives

TODO: @acrichto: passing vector types on the stack vs on function arguments.

What where the alternatives here? We discussed simd ABIs as a feature, shims,
... and many other things, and they all caused problems with trait methods,
function pointers, etc.

# Prior art
[prior-art]: #prior-art

All of this is implemented in `stdsimd` and can be used on nightly today via the
`std::simd` module. The `stdsimd` crate is an effort started by @burntsushi to
put the `rust-lang-nursery/simd` crate into a state suitable for stabilization.
The `rust-lang-nursery/simd` crate was mainly developed by @huonw and IIRC it is
heavily-inspired by Dart's SIMD which is from where the `f32x4` naming scheme
comes from.

# Unresolved questions
[unresolved]: #unresolved-questions

### Interaction with Cray vectors

The vector types proposed in this RFC are packed, that is, their size is fixed
at compile-time.

Many modern architectures support vector operations of run-time size, often
called Cray Vectors or scalable vectors. These include, amongst others, NecSX,
ARM SVE, RISC-V Vectors. These architectures have traditionaly relied on
auto-vectorization combined with support for explicit vectorization annotations,
but newer architectures like ARM SVE and RISC-V introduce explicit vectorization
intrinsics. 

This is an example adapted from this [ARM SVE
paper](https://developer.arm.com/hpc/arm-scalable-vector-extensions-and-application-to-machine-learning)
to pseudo-Rust:

```rust
/// Adds `c` to every element of the slice `src` storing the result in `dst`.
fn add_constant(dst: &mut [f64], src: &[f64], c: f64) {
    assert!(dst.len() == src.len());
    
    // Instantiate a dynamic vector (f64xN) with all lanes set to c:
    let vc: f64xN = f64xN::splat(c);
    
    // The number of lanes that each iteration of the loop can process
    // is unknown at compile-time (f64xN::lanes() is evaluated at run-time):
    for i in (0..src.len()).step_by_with(f64xN::lanes()) {
    
        // Instantiate a dynamic boolean vector with the
        // result of the predicate: `i + lane < src.len()`.
        // This boolean vector acts as a mask, so that elements 
        // "in-bounds" of the slice `src` are initialized to `true`,
        // while out-of-bounds elements contain `false`:
        let m: bxN = f64xN::while_lt(i, src.len());

        // Load the elements of the source using the mask:
        let vsrc: f64xN = f64xN::load(m, &src[i..]);
        
        // Add the vector with the constan using the mask:
        let vdst: f64xN = vsrc.add(m, vc);
        
        // Store the result back to memory using the mask:
        vdst.store(m, &mut dst[i..]);
    }
}
```

RISC-V proposes a model smilar in spirit, but not identical to the ARM SVE one.
It would not be surprising if other popular architectures offered similar but
not identical explicit vectorization models for Cray vectors in the future.

Notice that while the API of scalable vectors is similar to that of packed
vectors, there are some important differences

The number of lanes is only known at run-time. That is, Cray Vectors are types
of run-time size and it is unclear at this point of `mem::size_of::<f64xN>()`
will be able to return anything meaningful at compile-time or not (maybe they
might be "pointer sized", @eddyb?).

This affects how they are used. First, the induction loop variable is
incremented with a dynamic value. (@eddyb does a stride that changes across loop
iterations make sense here? Some architectures might do that?)

Second, in these architectures most operations require a mask that indicate
which elements are part of the vector and which aren't. The operations are not
performed on elements that aren't part of the vector. This encourages a
different API than that of packed vectors.

At these point in time it is still unclear how the current APIs actually work in
practice and designing a portable API for Cray vectors is an open research
problem. 

As shown above, such an API could be made to look "similar", but probably not
identical. This RFC does not prevent adding such an API in the future. 

Providing a portable API for both packed and Cray vectors is thus
out-of-scope for this RFC. 

At this point in time, whether packed and scalable vector types can interact
with each other is unknown (TODO: how does SVE interact with ASIMD/NEON? I can't
find anything about this online anywhere), and so is how this interaction would
look like iff they could interact.

### Half-float support

Many vector intrinsics on many architectures operates on vectors of half-floats
(`f16`) types. It is unclear what to do with these as this point since the `f16`
type is even less portable than the `{i,u}128` types.

Initially they will be explored as architecture-specific vector types. Whether
vectors of `f16` types can be made portable or not remains to be explored.

### 512-bit vector types and masks
