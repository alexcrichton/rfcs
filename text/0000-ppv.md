- Feature Name: `minimal_portable_packed_vector_types`
- Start Date: (fill me in with today's date, YYYY-MM-DD)
- RFC PR: (leave this empty)
- Rust Issue: (leave this empty)

# Summary
[summary]: #summary

This RFC adds portable packed SIMD vector types up to 256-bit.

Future RFCs will cover some of the following extensions as they become finished
in `stdsimd`:

* portable vector shuffles, gathers, and scatters
* boolean vectors, boolean reductions, portable vector comparisons
* 512-bit vector types and masks
* half-float (f16) vectors

# Motivation
[motivation]: #motivation

The `std::arch` module exposes architecture-specific SIMD types like `_m128` - a
128-bit wide SIMD vector type. How these bits are interpreted depends on the intrinsics
being used. For example, let's sum 8 `f32`s values using the SSE4.1 facilities
in the `std::arch` module. This is one way to do it
([playground](https://play.rust-lang.org/?gist=165e2886b4883ec98d4e8bb4d6a32e22&version=nightly)):

```rust
unsafe fn add_reduce(a: __m128, b: __m128) -> f32 {
    let c = _mm_hadd_ps(a, b);
    let c = _mm_hadd_ps(c, _mm_setzero_ps());
    let c = _mm_hadd_ps(c, _mm_setzero_ps());
    std::mem::transmute(_mm_extract_ps(c, 0))
}

fn main() {
    unsafe {
        let a = _mm_set_ps(1., 2., 3., 4.);
        let b = _mm_set_ps(5., 6., 7., 8.);
        let r = add_reduce(a, b);
        assert_eq!(r, 36.);
    }
}
```

Notice that:

* one has to put some effort to extrapolate from `add_reduce`'s signature what
  types of vectors it actually expects: "ok, add_reduce takes 128-bit vectors,
  returns a `f32`, ah! Those 128-bit vectors should probably contain 4 packed
  f32s because that's the only combination of `f32`s that fits in 128 bits!"
  
* it requires a lot of `unsafe` code: the intrinsics are unsafe, the intrinsic API
  relies on the user performing transmutes, constructing the vectors is unsafe, etc.

* it requires a lot of architecture specific knowledge: how the intrinsics are
  called, how they are used together
  
* this solution only works on `x86` or `x86_64` with SSE4.1 enabled, that is, it
  is not portable.

With portable packed vector types we can do much better
([playground](https://play.rust-lang.org/?gist=7fb4e3b6c711b5feb35533b50315a5fb&version=nightly)):

```rust
fn main() {
    let a = f32x4::new(1., 2., 3., 4.);
    let b = f32x4::new(5., 6., 7., 8.);
    let r = (a + b).sum();
    assert_eq!(r, 36.);
}
```

Note that Rust generates the same assembly instruction sequence in both situations:

```asm
  haddps xmm0, xmm1
  xorps xmm1, xmm1
  haddps xmm0, xmm1
  haddps xmm0, xmm1
```

Which reveals another key feature of these vector types: they add
zero-overhead - or we have a bug in the implementation that we must fix.

# Guide-level explanation
[guide-level-explanation]: #guide-level-explanation

This RFC introduces portable packed SIMD vector types and their minimal API. Here:

* portable: is the opposite of architecture-specific. These types work both
  correctly and efficiently on all architectures. They are a zero-overhead
  abstraction, that is, for the operations that these types support, one cannot
  write better code by hand (otherwise it is an implementation bug).
  
* packed: is the opposite of "scalable" or "Cray vectors". That is, these
  vector's have a compile-time fixed length. They can be stored inside structs,
  on the stack, and on the heap.
  
# Reference-level explanation
[reference-level-explanation]: #reference-level-explanation
  
## Vector types

The vector types are named according to the following scheme:

> {element_type}{element_width}x{number_of_lanes}

where the following element types are introduced by this RFC:

* `i`: signed integer
* `u`: unsigned integer
* `f`: float

So that `u16x8` reads "a SIMD vector of eight packed 16-bit wide unsigned
integers". The width of a vector can be computed by multiplying the
`{element_width}` times the `{number_of_lanes}`. For `u16x8`, 16 x 8 = 128, so
this vector type is 128 bits wide.

> Note: this nomenclature follows closely the nomenclature used by ARM in their
> vector types: <type><size>x<number of lanes>_t

This RFC proposes adding all vector types with sizes in range [16, 256] bit to
the `std::simd` module, that is:

* 16-bit wide vectors:
  * `i8x2`
  * `u8x2`
* 32-bit wide vectors:
  * `i8x4`
  * `u8x4`
  * `i16x2`
  * `u16x2`
* 64-bit wide vectors:
  * `i8x8`
  * `u8x8`
  * `i16x4`
  * `u16x4`
  * `i32x2`
  * `u32x2`
  * `f32x2`
* 128-bit wide vectors:
  * `i8x16`
  * `u8x16`
  * `i16x8`
  * `u16x8`
  * `i32x4`
  * `u32x4`
  * `f32x4`
  * `i64x2`
  * `u64x2`
  * `f64x2`

* 256-bit wide vectors:
  * `i8x32`
  * `u8x32`
  * `i16x16`
  * `u16x16`
  * `i32x8`
  * `u32x8`
  * `f32x8`
  * `i64x4`
  * `u64x4`
  * `f64x4`

Note that:

* half-float (`f16`) vectors (and half-floats in general) will need to be
  covered by future RFCs to support ARM/AArch64/PowerPC64/... intrinsics
* 512-bit wide vectors and masks will need to be covered once `std::arch` adds AVX-512 support
* vectors with 128-bit wide elements (`{i,u,f}128x1`, `{i,u,f}128x2`) will need
  to be covered once `std::arch` adds PowerPC support.

## API of portable packed SIMD vector types

### Traits overview

All signed integer, unsigned integer, and floating point vector types implement
the following traits:

* `Copy`
* `Clone`
* `Default`: zero-initializes the vector.
* `Debug`: formats the vector as `({}, {}, ...)`.
* `PartialEq<Self>`: performs a lane-wise comparison between two vectors and
  returns `true` if all lanes compare `true`.
* `PartialOrd<Self>`: performs a lane-wise comparison between two vectors and
  returns `true` if all lanes compare `true`. TODO: we should actually perform a
  lexicographical order here. Does this make sense? It's not going to be
  efficient. What we currently do IIRC is `a.lt(b).all()`.
* `From`/`Into`/`FromBits`/`IntoBits`: see the [lane-wise casts and bitwise
  conversions](#casts-and-conversions) section below.
* `Add<Output=Self>`, `Sub<Output=Self>`, `Mul<Output=Self>`, 
  `Div<Output=Self>`, `Rem<Output=Self>`, `AddAssign`, `SubAssign`, `MulAssign`, 
  `DivAssign`, `RemAssign`: lane-wise arithmetic operations.

All signed and unsigned integer vectors also implement:

* `Eq`: equivalent to `PartialEq<Self>`
* `Ord`: equivalent to `PartialOrd<Self>`
* `Hash`: equivalent to `Hash` for `[element_type; number_of_elements]`.
* `fmt::LowerHex`: formats the vector as hexadecimal.
* TODO: `fmt::UpperHex`: formats the vector as hexadecimal.
* TODO: `fmt::Octal`: formats the vector as an octal number.
* TODO: `fmt::Binary`: formats the vector as binary number.
* `Not<Output=Self>`,`BitAnd<Output=Self>`, `BitOr<Output=Self>`,
  `BitXor<<Output=Self>`, `BitAndAssign`, `BitOrAssign`,
  `BitXorAssign`: lane-wise bitwise operations.
* `Shl<*>`, `Shr<*>`, `ShlAssign<*>`,`ShrAssign<*>`: shifts for all literal
  integer types.

### Inherent Methods

#### Construction and element access

All portable vector types implement the following methods:

```rust
impl {element_type}{element_width}x{number_of_lanes} {

/// Creates a new instance of the vector from `number_of_lanes` 
/// values.
pub const fn new(args...) -> Self;

/// Returns the number of vector lanes.
pub const fn lanes() -> usize;

/// Constructs a new instance with each element initialized to
/// `value`.
pub const fn splat(value: element_type) -> Self;

/// Extracts the value at `index`.
///
/// # Panics
///
/// If `index >= Self::lanes()`.
pub fn extract(self, index: usize) -> element_type;

/// Extracts the value at `index`.
///
/// If `index >= Self::lanes()` the behavior is undefined.
pub unsafe fn extract_unchecked(self, index: usize) -> element_type;

/// Returns a new vector where the value at `index` is replaced by `new_value`.
///
/// # Panics
///
/// If `index >= Self::lanes()`.
#[must_use = implementation-defined]
pub fn replace(self, index: usize, new_value: $elem_ty) -> Self;

/// Returns a new vector where the value at `index` is replaced by `new_value`.
///
/// # Panics
///
/// If `index >= Self::lanes()`.
#[must_use = implementation-defined]
pub unsafe fn replace_unchecked(self, index: usize, 
                                new_value: element_type) -> Self;

}
```

TODO: @sunfish commented that `new` lowers to a memory load even when all
arguments are constant. This is inefficient, and probably something to be
discouraged (people should be doing a load here, or a sequence of replaces). 

OTOH @gnzlbg finds this useful, but that might be because it is useful inside
`stdsimd` to write tests and short examples, but arguably it is not something
used by high-performance code.

It might thus make sense to:

* have a way to construct these from compile-time constants
* use that way to construct these when all arguments to `new` are constants
* either require the arguments of `new` to be constants, or add a way that
  requires this, like a literal syntax for the simd types: `f32x4(1., 2., 3.,
  4.)` that requires all input arguments to be compile-time constants.
  
Skimming through `stdsimd` I can't find a single place where `new` is used with
non-constant arguments.

TODO: @sunfish mentions that he is not aware of any hardware that supports
dynamic indexing into vector elements:

> It's not horrible, but it's not a SIMD operation. It's a memory operation, and
> Rust can already do it. Just create a [f32; 4], store the vector there, index it
> yourself, and the compiler should generate the same code. If this were a common
> operation, it'd be nice to have a convenient API to do it automatically, but as
> far as I know it isn't (otherwise we might have hardware for it ;-)). And that
> way, you don't need the `_unchecked` versions, because users can do whatever
> unsafe thing they want with memory themselves.

So in a nutshell we might want to require the arguments of `new` and
`extract`/`store`/`replace` to be `const`s. The problem would be that we can't
use the `constify!` techniques here that the immediate-mode arguments use in
`std::arch`, so maybe we'll need to make these be macros for now.

#### Lane-wise casts and bitwise conversions
[casts-and-conversions]: #casts-and-conversions

The portable vector types implement lane-wise and bitwise casts.

##### Lane-wise casts

TODO: `From`/`Into` are lossless. What we actually want here is `as` casts.
Implementing `From`/`Into`` here with a different meaning is not going to
survive the RFC process. The alternative was `.as_f32x4()` methods, but that was
painful to implement and use.

Vectors with identical number of lanes can be converted into each other by
performing lane-wise `as` casts provided by the `From` and `Into` traits. 

For example:

```rust
let x = f32x2::new(1., 2.);
let y: f64x2 = x.into();
```

These conversions are provided across all portable vector types having the same
number of lanes.

##### Bitwise casts

TODO: `f32::from_bits` takes a `u32`, if we were to add these as methods we'd
need to add a bunch of them for each type as well `__m128::from_bits_{f32x4,
...}`..

To interface with the architecture-specific vector types and intrinsics it is
often required to perform lossless bitwise casts from one type to another.

In an analogous way to the `f32::from_bits` method and the `From` and `Into`
traits, these conversions are performed via the reflexive `FromBits`/`IntoBits`
traits. These conversions have run-time zero-overhead.

For example:

```rust
let x = f32x4::new(1., 2., 3., 4.);
let y: __m128 = x.into_bits();
```

These conversions are provided across all portable vector types of the same
width, and across the portable vector types and the architecture-specific vector
types.

TODO: @sunfish gave the following comment: If the bitwise casts can convert
between vectors with different numbers of elements (eg. between `i8x16` and
`i16x8`), the results will depend on the endianness of the machine.

#### Loads and Stores

All portable vector types implement the following methods:

```rust
impl {element_type}{element_width}x{number_of_lanes} {

/// Writes the values of the vector to the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not
/// aligned to an `align_of::<Self>()` boundary.
pub fn store_aligned(self, slice: &mut [element_type]);

/// Writes the values of the vector to the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()`.
pub fn store_unaligned(self, slice: &mut [element_type]);

/// Writes the values of the vector to the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not
/// aligned to an `align_of::<Self>()` boundary, the behavior is
/// undefined.
pub unsafe fn store_aligned_unchecked(self, slice: &mut [element_type]);

/// Writes the values of the vector to the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` the behavior is undefined.
pub unsafe fn store_unaligned_unchecked(self, slice: &mut [element_type]);

/// Instantiates a new vector with the values of the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not aligned
/// to an `align_of::<Self>()` boundary.
pub fn load_aligned(slice: &[element_type]) -> Self

/// Instantiates a new vector with the values of the `slice`.
///
/// # Panics
///
/// If `slice.len() < Self::lanes()`.
pub fn load_unaligned(slice: &[element_type]) -> Self;

/// Instantiates a new vector with the values of the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` or `&slice[0]` is not aligned
/// to an `align_of::<Self>()` boundary, the behavior is undefined.
pub unsafe fn load_aligned_unchecked(slice: &[element_type]) -> Self;

/// Instantiates a new vector with the values of the `slice`.
///
/// # Precondition
///
/// If `slice.len() < Self::lanes()` the behavior is undefined.
pub unsafe fn load_unaligned_unchecked(slice: &[element_type]) -> Self;

}
```

TODO: @sunfish gave the following comment: need to document how big-endian does
things.

#### Arithmetic reductions 

All portable vector types implement the following methods:

```rust
impl {element_type}{element_width}x{number_of_lanes} {

/// Horizontal sum of the vector elements.
pub fn sum(self) -> element_type;

/// Horizontal product of the vector elements.
pub fn product(self) -> element_type;

}
```

##### Semantics for integer vectors

If integer overflow occurs the result is the mathematical result modulo `2^n`. 

##### Semantics for floating point vectors.

The floating-point horizontal reductions are ordered (TODO), that is,
equivalent to the following Rust code:

```rust
// TODO
```

TODO: @sunfish commented that ordered reductions can be made pretty efficient by
doing a tree reduction (VPADD on NEON, HADDPS on x86): `(x[0]+x[1])+(x[2]+x[3])`
but nothe that this is not equivalent to `x[0] + x[1] + x[2] + x[3]` which is
what the following code would do:

```rust
let mut val: f32 = vec.extract(0);
for i in 1..f32x4::lanes() {
    val += vec.extract(i);
}
assert_eq!(val, vec.sum()); // MAYBE FAILS
```

TODO: AFAICT the LLVM reductions don't do a tree reduction, but I should
triple-check this.


#### Bitwise reductions 

All signed and unsigned integer vectors implement the following methods:

```rust
impl {element_type}{element_width}x{number_of_lanes} {

/// Horizontal bitwise `and` of the vector elements.
pub fn and(self) -> element_type;

/// Horizontal bitwise `or` of the vector elements.
pub fn or(self) -> element_type;

/// Horizontal bitwise `xor` of the vector elements.
pub fn xor(self) -> element_type;

}
```

#### Min/Max reductions

All portable vector types implement the following methods:

```rust
impl {element_type}{element_width}x{number_of_lanes} {
            
/// Value of the largest element in the vector.
pub fn max(self) -> element_type;

/// Value of the smallest element in the vector.
pub fn min(self) -> element_type;

}
```

##### Semantics for floating-point vectors

If the vector contains `NaN`s the result is unspecified (TODO). 

TODO: @sunfish would prefer to handle `NaNs` in Min/Max by default and, if
desired, adding a version that optimizes for the non-`NaN`s case as an
alternative.

If the vector contains a `NaN`, then both `min`/`max` would return it:

```rust
let nan: f32 = 1. / 0.;
let vec = f32x4::new(1., 2., nan, 3.);
assert_eq!(vec.max() as u32, nan as u32);
assert_eq!(vec.min() as u32, nan as u32);
```

The problem is what do we do when the vector has multiple `NaN`s ? That is, given

```rust
let nan_0: f32 = ...some nan...;
let nan_1: f32 = ...some nan...;
let nan_2: f32 = ...some nan...;
let nan_3: f32 = ...some nan...;
assert!(nan_0 as u32 != nan_1 as u32 && nan_0 as u32 != nan_2 as u32 &&
        nan_0 as u32 != nan_3 as u32);
assert!(nan_1 as u32 != nan_2 as u32 && nan_1 as u32 != nan_3 as u32);
assert!(nan_2 as u32 != nan_3 as u32);
let vec = f32x4::new(nan_0, nan_1, nan_2, nan_3);
asset_eq!(vec.max() as u32, ???);
asset_eq!(vec.min() as u32, ???);
```

We should check how to implement these manually for, e.g., f32x4 on AArch64,
x86_64, and PowerPC64 and see how the fastest code behaves in the presence of
NaNs, and what would it take to homogenize the behavior here. The results would
then belong in the rationale section.

# TODO: ABI 

TODO: @acrichto: IIRC currently the Rust ABI for SIMD types always passes
vector types by pointers. This avoids the issues mentioned in:
https://github.com/rust-lang/rust/issues/44367

How exactly this work is pretty important and should be explained here. 

Then we have the issue that SIMD types in `extern` functions might not be sound.
IIRC this is currently disallowed. Is there an issue for this? 

# Drawbacks
[drawbacks]: #drawbacks

The main drawback is that in a future where Rust has multiple backends,
providing this types will take some work. Most backeds support vector types in
one form or another, and if the backend supports the architecture-specific
vector types exposed by `std::arch` chances are that very little work will
actually be required to support this.

# Rationale and alternatives
[alternatives]: #alternatives

TODO: @acrichto: passing vector types on the stack vs on function arguments.

What where the alternatives here? We discussed simd ABIs as a feature, shims,
... and many other things, and they all caused problems with trait methods,
function pointers, etc.

# Prior art
[prior-art]: #prior-art

All of this is implemented in `stdsimd` and can be used on nightly today via the
`std::simd` module. The `stdsimd` crate is an effort started by @burntsushi to
put the `rust-lang-nursery/simd` crate into a state suitable for stabilization.
The `rust-lang-nursery/simd` crate was mainly developed by @huonw and IIRC it is
heavily-inspired by Dart's SIMD which is from where the `f32x4` naming scheme
comes from.

# Unresolved questions
[unresolved]: #unresolved-questions

TBD.
